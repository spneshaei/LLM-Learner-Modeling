{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "\n",
    "This file calculates the baselines used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, mean_squared_error\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# From https://galea.medium.com/how-to-love-jsonl-using-json-line-format-in-your-workflow-b6884f65175b\n",
    "\n",
    "from json import JSONEncoder\n",
    "\n",
    "class MyEncoder(JSONEncoder):\n",
    "        def default(self, o):\n",
    "            return o.__dict__ \n",
    "        \n",
    "import json\n",
    "\n",
    "def dump_jsonl(data, output_path, append=False):\n",
    "    \"\"\"\n",
    "    Write list of objects to a JSON lines file.\n",
    "    \"\"\"\n",
    "    mode = 'a+' if append else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8') as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False, cls=MyEncoder)\n",
    "            f.write(json_record + '\\n')\n",
    "    print('Wrote {} records to {}'.format(len(data), output_path))\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data\n",
    "\n",
    "class JSONLDataObject:\n",
    "    prompt = \"\"\n",
    "    completion = \"\"\n",
    "\n",
    "    def __init__(self, prompt, completion):\n",
    "        self.prompt = prompt\n",
    "        self.completion = completion\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.prompt, self.completion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean baseline\n",
    "\n",
    "df = pd.DataFrame(columns=[\"dataset\", \"approach\", \"balanced_accuracy\", \"raw_accuracy\", \"f1\", \"precision\", \"recall\", \"auc\", \"rmse\"])\n",
    "\n",
    "for dataset_name in [\"statics\", \"assistments09\", \"assistments17\"]:\n",
    "    for approach in [\"minimal\", \"extended\"]:\n",
    "        test_data = load_jsonl(f\"jsonl_files/{dataset_name}-{approach}-test.jsonl\")\n",
    "        train_data = load_jsonl(f\"jsonl_files/{dataset_name}-{approach}-train.jsonl\")\n",
    "        number_of_correct = 0\n",
    "        number_total = 0\n",
    "        for i in range(len(train_data)):\n",
    "            if train_data[i][\"completion\"] == \"CORRECT\":\n",
    "                number_of_correct += 1\n",
    "            number_total += 1\n",
    "        assert number_total == len(train_data)\n",
    "        majority_label = (1 if number_of_correct > (number_total - number_of_correct) else 0)\n",
    "        prob_of_correct = 1.0 * number_of_correct / number_total\n",
    "        \n",
    "\n",
    "        all_completions = []\n",
    "        all_original = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Mean baseline\n",
    "        for i in range(len(test_data)):\n",
    "            all_completions.append(majority_label)\n",
    "            all_original.append(1 if test_data[i][\"completion\"] == \"CORRECT\" else 0)\n",
    "            all_probs.append(prob_of_correct)\n",
    "\n",
    "        balanced_accuracy = balanced_accuracy_score(all_original, all_completions)\n",
    "        raw_accuracy = accuracy_score(all_original, all_completions)\n",
    "        f1 = f1_score(all_original, all_completions)\n",
    "        if f1 == 0.0:\n",
    "            print(all_original)\n",
    "            print(all_completions)\n",
    "        precision = precision_score(all_original, all_completions)\n",
    "        recall = recall_score(all_original, all_completions)\n",
    "        auc = roc_auc_score(all_original, all_probs)\n",
    "        mse = mean_squared_error(all_original, all_probs)\n",
    "        rmse = math.sqrt(mse)\n",
    "        \n",
    "\n",
    "        df = df.append({\"dataset\": dataset_name, \"approach\": approach, \"balanced_accuracy\": balanced_accuracy, \"raw_accuracy\": raw_accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"auc\": auc, \"rmse\": rmse}, ignore_index=True)\n",
    "\n",
    "df.to_csv(\"mean_baseline.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaP Full baseline\n",
    "\n",
    "df = pd.DataFrame(columns=[\"dataset\", \"approach\", \"balanced_accuracy\", \"raw_accuracy\", \"f1\", \"precision\", \"recall\", \"auc\", \"rmse\"])\n",
    "\n",
    "for dataset_name in [\"statics\", \"assistments09\", \"assistments17\"]:\n",
    "    for approach in [\"minimal\", \"extended\"]:\n",
    "        test_data = load_jsonl(f\"jsonl_files/{dataset_name}-{approach}-test.jsonl\")\n",
    "        number_of_correct = 0\n",
    "        number_total = 0\n",
    "        for i in range(len(test_data)):\n",
    "            if test_data[i][\"completion\"] == \"CORRECT\":\n",
    "                number_of_correct += 1\n",
    "            number_total += 1\n",
    "        assert number_total == len(test_data)\n",
    "        majority_label = (1 if number_of_correct > (number_total - number_of_correct) else 0)\n",
    "        if majority_label == 1:\n",
    "            prob_of_correct = 1.0 * number_of_correct / number_total\n",
    "        else:\n",
    "            prob_of_correct = 1.0 * (number_total - number_of_correct) / number_total\n",
    "\n",
    "        all_completions = []\n",
    "        all_original = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Mean baseline\n",
    "        for i in range(len(test_data)):\n",
    "            lines = test_data[i][\"prompt\"].split(\"\\n\")\n",
    "            start_index = 0 if approach == \"minimal\" else 3\n",
    "            correct_until_now = int(lines[start_index].replace(\"Total correct until now: \", \"\").replace(\" \", \"\"))\n",
    "            wrong_until_now = int(lines[start_index + 1].replace(\"Total wrong until now: \", \"\").replace(\" \", \"\"))\n",
    "            if wrong_until_now == correct_until_now:\n",
    "                all_completions.append(random.choice([0, 1]))\n",
    "                all_probs.append(0.5)\n",
    "            else:\n",
    "                all_completions.append(1 if correct_until_now > wrong_until_now else 0)\n",
    "                all_probs.append(1.0 * correct_until_now / (correct_until_now + wrong_until_now))\n",
    "            all_original.append(1 if test_data[i][\"completion\"] == \"CORRECT\" else 0)\n",
    "            \n",
    "        balanced_accuracy = balanced_accuracy_score(all_original, all_completions)\n",
    "        raw_accuracy = accuracy_score(all_original, all_completions)\n",
    "        f1 = f1_score(all_original, all_completions)\n",
    "        precision = precision_score(all_original, all_completions)\n",
    "        recall = recall_score(all_original, all_completions)\n",
    "        auc = roc_auc_score(all_original, all_probs)\n",
    "        mse = mean_squared_error(all_original, all_probs)\n",
    "        rmse = math.sqrt(mse)\n",
    "\n",
    "        df = df.append({\"dataset\": dataset_name, \"approach\": approach, \"balanced_accuracy\": balanced_accuracy, \"raw_accuracy\": raw_accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"auc\": auc, \"rmse\": rmse}, ignore_index=True)\n",
    "\n",
    "df.to_csv(\"nap_full_baseline.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaP Full Skill baseline\n",
    "\n",
    "df = pd.DataFrame(columns=[\"dataset\", \"approach\", \"balanced_accuracy\", \"raw_accuracy\", \"f1\", \"precision\", \"recall\", \"auc\", \"rmse\"])\n",
    "\n",
    "for dataset_name in [\"statics\", \"assistments09\", \"assistments17\"]:\n",
    "    for approach in [\"extended\"]:\n",
    "        test_data = load_jsonl(f\"jsonl_files/{dataset_name}-{approach}-test.jsonl\")\n",
    "        train_data = load_jsonl(f\"jsonl_files/{dataset_name}-{approach}-train.jsonl\")\n",
    "\n",
    "        all_completions = []\n",
    "        all_original = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Mean baseline\n",
    "        for i in range(len(test_data)):\n",
    "            lines = test_data[i][\"prompt\"].split(\"\\n\")\n",
    "            correct_until_now = int(lines[1].split(\":\")[1].replace(\" \", \"\"))\n",
    "            wrong_until_now = int(lines[2].split(\":\")[1].replace(\" \", \"\"))\n",
    "            if wrong_until_now == correct_until_now:\n",
    "                all_completions.append(random.choice([0, 1]))\n",
    "                all_probs.append(0.5)\n",
    "            else:\n",
    "                all_completions.append(1 if correct_until_now > wrong_until_now else 0)\n",
    "                all_probs.append(1.0 * correct_until_now / (correct_until_now + wrong_until_now))\n",
    "            all_original.append(1 if test_data[i][\"completion\"] == \"CORRECT\" else 0)\n",
    "            \n",
    "        balanced_accuracy = balanced_accuracy_score(all_original, all_completions)\n",
    "        raw_accuracy = accuracy_score(all_original, all_completions)\n",
    "        f1 = f1_score(all_original, all_completions)\n",
    "        precision = precision_score(all_original, all_completions)\n",
    "        recall = recall_score(all_original, all_completions)\n",
    "        auc = roc_auc_score(all_original, all_probs)\n",
    "        mse = mean_squared_error(all_original, all_probs)\n",
    "        rmse = math.sqrt(mse)\n",
    "\n",
    "        df = df.append({\"dataset\": dataset_name, \"approach\": approach, \"balanced_accuracy\": balanced_accuracy, \"raw_accuracy\": raw_accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"auc\": auc, \"rmse\": rmse}, ignore_index=True)\n",
    "\n",
    "df.to_csv(\"nap_full_skill_baseline.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
