{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Inference\n",
    "\n",
    "This file uses the GPT-3.5 API to perform zero-shot inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tenacity\n",
    "from retry import retry\n",
    "import backoff \n",
    "import openai\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "dataset_name = \"assistments09\" # statics, assistments09, assistments17\n",
    "approach = \"minimal\" # minimal, extended\n",
    "\n",
    "API_KEY = \"\" # ANON\n",
    "ORGANIZATION = \"\" # ANON\n",
    "openai.api_key = API_KEY\n",
    "openai.organization = ORGANIZATION\n",
    "client = OpenAI(api_key=API_KEY, organization=ORGANIZATION)\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://galea.medium.com/how-to-love-jsonl-using-json-line-format-in-your-workflow-b6884f65175b\n",
    "\n",
    "from json import JSONEncoder\n",
    "\n",
    "class MyEncoder(JSONEncoder):\n",
    "        def default(self, o):\n",
    "            return o.__dict__ \n",
    "\n",
    "import json\n",
    "\n",
    "def dump_jsonl(data, output_path, append=False):\n",
    "    \"\"\"\n",
    "    Write list of objects to a JSON lines file.\n",
    "    \"\"\"\n",
    "    mode = 'a+' if append else 'w'\n",
    "    with open(output_path, mode, encoding='utf-8') as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False, cls=MyEncoder)\n",
    "            f.write(json_record + '\\n')\n",
    "    print('Wrote {} records to {}'.format(len(data), output_path))\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    \"\"\"\n",
    "    Read list of objects from a JSON lines file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data\n",
    "\n",
    "class JSONLDataObject:\n",
    "    prompt = \"\"\n",
    "    completion = \"\"\n",
    "\n",
    "    def __init__(self, prompt, completion):\n",
    "        self.prompt = prompt\n",
    "        self.completion = completion\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.prompt, self.completion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_jsonl(f'jsonl_files/{dataset_name}-{approach}-test.jsonl')\n",
    "print(len(test_data), test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_completions_temp_0 = []\n",
    "all_original_temp_0 = []\n",
    "all_logprobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(test_data) - 2401) as pbar:\n",
    "    for i in range(2401, len(test_data)):\n",
    "        prompt = test_data[i]['prompt']\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an instructor and want to predict whether a student will get a question CORRECT or WRONG. The only information you have is the student's previous answers to a series of related questions. You know how many questions they got CORRECT and how many they got WRONG. Based on this information, you should make a prediction by outputting a single word: CORRECT if you think the student will answer the next question correctly, and WRONG if you think the student will answer the next question wrong. Output no other word at all, this is very important. Try to estimate the knowledge of the student before making your prediction.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            logprobs=True,\n",
    "            top_logprobs=5\n",
    "            \n",
    "        )\n",
    "        all_completions_temp_0.append(response.choices[0].message.content.split(' ')[0].strip())\n",
    "        all_original_temp_0.append(prompt)\n",
    "        all_logprobs.append(response.choices[0].logprobs)\n",
    "        pbar.update(1)\n",
    "\n",
    "unique_values = list(set(all_completions_temp_0))\n",
    "print(unique_values) # should be ['CORRECT', 'WRONG']\n",
    "print(len(all_completions_temp_0), len(all_original_temp_0), len(all_logprobs), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example test data\", test_data[0])\n",
    "\n",
    "true_completions = []\n",
    "for i in range(len(test_data)):\n",
    "    true_completions.append(test_data[i]['completion'])\n",
    "\n",
    "all_completions_cleaned = []\n",
    "for completion in all_completions_temp_0:\n",
    "    if 'correct' in completion.lower():\n",
    "        all_completions_cleaned.append('CORRECT')\n",
    "    else:\n",
    "        all_completions_cleaned.append('WRONG')\n",
    "\n",
    "\n",
    "# convert all CORRECT to 1 and all WRONG to 0\n",
    "true_completions_binary = []\n",
    "for completion in true_completions:\n",
    "    if 'correct' in completion.lower():\n",
    "        true_completions_binary.append(1)\n",
    "    else:\n",
    "        true_completions_binary.append(0)\n",
    "\n",
    "all_completions_binary = []\n",
    "for completion in all_completions_cleaned:\n",
    "    if 'correct' in completion.lower():\n",
    "        all_completions_binary.append(1)\n",
    "    else:\n",
    "        all_completions_binary.append(0)\n",
    "\n",
    "print(list(set(true_completions_binary))) # should be [0, 1]\n",
    "print(list(set(all_completions_binary))) # should be [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(all_completions_binary.count(0))\n",
    "print(all_completions_binary.count(1))\n",
    "\n",
    "print(\"Balanced accuracy =>\", \"{:.4f}\".format(balanced_accuracy_score(true_completions_binary, all_completions_binary)))\n",
    "print(\"Raw Accuracy =>\", \"{:.4f}\".format(accuracy_score(true_completions_binary, all_completions_binary)))\n",
    "print(\"F1 =>\", \"{:.4f}\".format(f1_score(true_completions_binary, all_completions_binary)))\n",
    "print(\"Precision =>\", \"{:.4f}\".format(precision_score(true_completions_binary, all_completions_binary)))\n",
    "print(\"Recall =>\", \"{:.4f}\".format(recall_score(true_completions_binary, all_completions_binary)))\n",
    "\n",
    "with open(f'inference_results/feb16pm-{dataset_name}-{approach}-nospace-zero-shot-true_completions_binary.txt', 'w') as f:\n",
    "    for item in true_completions_binary:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(f'inference_results/feb16pm-{dataset_name}-{approach}-nospace-zero-shot-all_completions_binary.txt', 'w') as f:\n",
    "    for item in all_completions_binary:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# save logprobs to a file\n",
    "with open(f'inference_results/feb16pm-{dataset_name}-{approach}-nospace-zero-shot-logprobs.txt', 'w') as f:\n",
    "    for item in all_logprobs:\n",
    "        f.write(\"%s\\n\" % str(item))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
